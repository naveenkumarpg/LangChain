{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7d2a48d",
   "metadata": {},
   "source": [
    "#### Sample test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90ec18ec-cac4-4b71-8827-ff2ea04fa78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello ..!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "print('Hello ..!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeb8167",
   "metadata": {},
   "source": [
    "#### Injecting api key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5a5f187-f68c-410c-9948-69d4fd9f77a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "openai.apikey = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b6ab6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cea1108",
   "metadata": {},
   "source": [
    "#### Simple invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d97d2ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ChatOpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Creating instance of LLM\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Invoking LLM with prompt text\u001b[39;00m\n\u001b[1;32m      8\u001b[0m output \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWrite a program to generate 10 fib numbers using python\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/Users/Naveen/Github/LangChain/virtualEnv/myenv/lib/python3.12/site-packages/langchain_core/load/serializable.py:113\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Users/Naveen/Github/LangChain/virtualEnv/myenv/lib/python3.12/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ChatOpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "#Importing Langchain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Creating instance of LLM\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "# Invoking LLM with prompt text\n",
    "output = llm.invoke('Write a program to generate 10 fib numbers using python')\n",
    "\n",
    "#Printing the output\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d32c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI()\n",
    "output = llm.invoke('What are the seven wonders of the world')\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b164b30e",
   "metadata": {},
   "source": [
    "## Prompt Template\n",
    "which will alow to structure the request in a specific fashion with prefilled text called prompt template, using this templates, you can reuse the prompt with simple parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cae6f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.9, model=)\n",
    "\n",
    "template = '''You are an experience virologist. Write me a few sentences about {virus} virus in {language}'''\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "samplePrompt = prompt_template.format(virus='covid', language='English')\n",
    "\n",
    "response = llm.invoke(samplePrompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07e48e5",
   "metadata": {},
   "source": [
    "### Chat Prompt templates\n",
    "chat prompt templates are used to talk to llm's in conversation manner, to keep track of previous request/responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be497809",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content='You respond only in JSON format'),\n",
    "    HumanMessagePromptTemplate.from_template('{prompt}')\n",
    "])\n",
    "\n",
    "messages = chat_template.format(prompt='Top 10 countries in Asia with population')\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2544fb7",
   "metadata": {},
   "source": [
    "### Simple chain\n",
    "In LangChain, a chain is a series of automated actions that are executed in a defined order to provide context-aware responses to a user's query. Chains are made up of multiple interconnected components, such as calls to language models (LLMs), external tools, or data preprocessing steps. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321f4390",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "prompt_template = PromptTemplate.from_template('You are an experience virologist. Write me a few sentences about {virus} virus in {language}')\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm, \n",
    "    prompt=prompt_template,\n",
    "    verbose=True ## adding verbose will give additional details like state and mode details\n",
    "    )\n",
    "\n",
    "print(chain)\n",
    "\n",
    "response = chain.invoke({'virus': 'HSV', \"language\":\"English\"})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33697a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "template = 'What is the capital of {country}, give me list of the places to visit in the country with details about the place as well, give the response in bullet points'\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "country  = input('Enter a country name : ');\n",
    "\n",
    "response = chain.invoke({\"country\":country})\n",
    "\n",
    "print(response['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435d1534",
   "metadata": {},
   "source": [
    "## Sequential chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e582b238",
   "metadata": {},
   "source": [
    "### Simple sequential chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2766f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n",
    "\n",
    "# Single chain\n",
    "# First chain\n",
    "llm_one = ChatOpenAI(temperature=0.5, model='gpt-3.5-turbo')\n",
    "template_one = PromptTemplate.from_template(template=\"You are an experienced programmer, Write a function to implement the {concept}\")\n",
    "chain_one = LLMChain(llm=llm_one, prompt=template_one)\n",
    "\n",
    "# Second chain\n",
    "llm_two = ChatOpenAI(temperature=1.5, model='gpt-4-turbo-preview')\n",
    "template_two = PromptTemplate.from_template(template=\"Given this code {code}, explain it as detail as possible with comments\")\n",
    "chain_two = LLMChain(llm=llm_two, prompt=template_two)\n",
    "\n",
    "combinedChains = SimpleSequentialChain(chains=[chain_one, chain_two], verbose=True)\n",
    "chainsResponse = combinedChains.invoke('linear regression')\n",
    "\n",
    "print(chainsResponse['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9165192a",
   "metadata": {},
   "source": [
    "### Wrong Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dcd8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI()\n",
    "output = llm.invoke('Calculate this 5.1**7.3')\n",
    "print(output.content) \n",
    "# this answer is wrong as LLM is approximating the answer. we do the same with Agents now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db114cd",
   "metadata": {},
   "source": [
    "# Agents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f5c845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.utilities import PythonREPL\n",
    "python_repl = PythonREPL()\n",
    "python_repl.run('print([n for n in range(1,100) if n% 13 ==0])')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d3af96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.agents.agent_toolkits import create_python_agent\n",
    "from langchain_experimental.tools.python.tool import PythonAstREPLTool\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model='gpt-4-turbo-preview')\n",
    "\n",
    "agent_executor = create_python_agent(\n",
    "    llm=llm,\n",
    "    tool=PythonAstREPLTool(),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "response = agent_executor.invoke('Calculate this 5.1**7.3 ')\n",
    "print(response['output'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d97b89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.agents.agent_toolkits import create_python_agent\n",
    "from langchain_experimental.tools.python.tool import PythonAstREPLTool\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model='gpt-4-turbo-preview')\n",
    "\n",
    "agent_executor = create_python_agent(\n",
    "    llm=llm,\n",
    "    tool=PythonAstREPLTool(),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "response = agent_executor.invoke('Calculate the square root of the factorial of 12 and display it with decimal points')\n",
    "\n",
    "print(response['output'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
